import asyncio
from sqlalchemy.ext.asyncio import AsyncSession
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import AsyncQdrantClient
from qdrant_client.http.models import (
    SearchParams,
    Filter,
    FieldCondition,
    MatchText,
    MatchAny,
    MatchValue,
)
import tiktoken
import numpy as np
from openai import AsyncOpenAI
from ..database.database import AsyncSessionLocal
from ..database.models import Document, DocumentChunk
from ..config import COLLECTION_NAME, LLM_MODEL, DEEPSEEK_API_KEY, QDRANT_HOST, EMBEDDING_MODEL_NAME, RESERVE_LLM_MODEL
from src.app.logging_config import get_logger
from google import genai
from google.genai.types import GenerateContentConfig

logger = get_logger(__name__)


# Variables check
if (LLM_MODEL is None) or (COLLECTION_NAME is None) or (DEEPSEEK_API_KEY is None) or (QDRANT_HOST is None) or (EMBEDDING_MODEL_NAME is None):
    raise ValueError("ะะตัะตะผะตะฝะฝัะต ะฝะต ะฝะฐะนะดะตะฝั. ะัะพะฒะตัััะต ัะฐะนะป.env.")


# =====================================================================
# ะกะตัะฒะธั ะฒะตะบัะพัะธะทะฐัะธะธ ะทะฐะฟัะพัะฐ
class QueryEmbeddingService:
    def __init__(self, model_name: str):
        # ะะฐะณััะทะบะฐ ััะถะตะปะพะณะพ ัะตััััะฐ (SentenceTransformer) ะพะดะธะฝ ัะฐะท
        self.__model = SentenceTransformer(model_name)

    async def vectorize_query(self, query: str) -> list[float]:
        """ะะตะบัะพัะธะทัะตั ะพะดะธะฝ ัะตะบััะพะฒัะน ะทะฐะฟัะพั ะดะปั ะฟะพะธัะบะฐ."""
        logger.info("Vectorizing user query...")
        query_embedding = await asyncio.to_thread(
            self.__model.encode,
            [query],
            normalize_embeddings=True,
            convert_to_tensor=False
        )
        logger.info("User query vectorized successfully.")
        return query_embedding.tolist()[0]
    

# Semantic search in Qdrant
class QueryQdrantClient:
    def __init__(self, host: str, collection_name: str):

        self.__client = AsyncQdrantClient(url=host, timeout=60)
        self.__collection_name = collection_name
        self.__cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    @staticmethod
    def _softmax(x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum(axis=0)

    
    async def semantic_search(self, query_vector: list[float], user_query: str, limit_k: int = 30):
        """Performs a true hybrid search in Qdrant by combining results from separate vector and full-text queries."""
        logger.info("Performing hybrid search in Qdrant...")

        # 1. Perform vector search
        vector_search_result = await self.__client.query_points(
            collection_name=self.__collection_name,
            query=query_vector,
            limit=limit_k,
            with_payload=True,
            with_vectors=False,
            score_threshold=0.5,
            search_params=SearchParams(
                hnsw_ef=200
            )
        )
        vector_candidates = vector_search_result.points
        if not vector_candidates:
            logger.warning("Vector search returned no candidates. Aborting.")
            return [], None, 0
        
        max_score = vector_candidates[0].score if vector_candidates else 0
        logger.info(f"Vector search found {len(vector_candidates)} candidates. Max score: {max_score:.4f}")

        # 2. Identify the top document ID from the most relevant chunk
        top_candidate = vector_candidates[0]
        if not top_candidate.payload or "document_id" not in top_candidate.payload:
            logger.error(
                "Top vector candidate is missing 'document_id'. Aborting."
            )
            return [], None, 0
        
        target_document_id = int(top_candidate.payload["document_id"])
        logger.info(f"Top document identified: document_id='{target_document_id}'")

        # # 2. Perform full-text search
        # user_query_words = user_query.lower().split()
        # logger.info(f"Executing full-text search with filter: document_id='{target_document_id}', words={user_query_words}")
       
        # text_search_result, _ = await self.__client.scroll(
        #     collection_name=self.__collection_name,
        #     scroll_filter=Filter(
        #         must= [
        #             FieldCondition(
        #                 key="document_id",
        #                 match=MatchValue(value=target_document_id),
        #             ),
        #             FieldCondition(
        #                 key="content",
        #                 match=MatchAny(any=user_query_words),
        #             ),
        #         ]
        #     ),
        #     limit=limit_k,
        #     with_payload=True,
        #     with_vectors=False,
        # )
        # text_candidates = text_search_result
        # logger.info(
        #     f"Full-text search found {len(text_candidates)} candidates within document '{target_document_id}'."
        # )
        

        # 3. Combine and deduplicate results
        combined_candidates = {
            c.id: c
            for c in vector_candidates
            if c.payload and c.payload.get("document_id") == target_document_id
        }

        # for candidate in text_candidates:
        #     combined_candidates[candidate.id] = candidate
        
        candidates = list(combined_candidates.values())
        logger.info(
            f"Found {len(candidates)} unique candidates from document '{target_document_id}'. Starting reranking..."
        )
        
        if not candidates:
            logger.warning("No candidates to rerank.")
            return [], None, 0
        
        # 4. Rerank the combined candidates
        # Filter out candidates without 'content' in payload to prevent KeyError
        rerank_pairs = [
            [user_query, candidate.payload["content"]]
            for candidate in candidates
            if candidate.payload and "content" in candidate.payload
        ]
        if not rerank_pairs:
            logger.warning(
                "No valid candidates for reranking after content check."
            )
            return [], None, 0
            
        reranked_scores = await asyncio.to_thread(
            self.__cross_encoder.predict, rerank_pairs
        )
        # Normalize scores and assign them to candidates
        normalized_scores = self._softmax(reranked_scores)
        for candidate, score in zip(candidates, normalized_scores):
            candidate.score = score
        
        # Sort by the new reranked score
        candidates.sort(key=lambda x: x.score, reverse=True)

  
        # 6. Select the top N chunks for the final context
        top_relevant_chunks = candidates[:5]
        target_chunk_ids = [
            p.payload["chunk_id"]
            for p in top_relevant_chunks
            if p.payload and "chunk_id" in p.payload
        ]
        if not target_chunk_ids:
            logger.error("Selected chunks are missing 'chunk_id'.")
            return [], None, 0
        
        # # 7: Request neighboring chunks (+1 and -1)
        # neighbor_chunk_ids = set()
        # for chunk_id in target_chunk_ids:
        #     if chunk_id > 0:
        #         neighbor_chunk_ids.add(chunk_id - 1)
        #     neighbor_chunk_ids.add(chunk_id + 1)

        # # Collect all required chunk IDs
        # all_required_chunk_ids = target_chunk_ids.union(neighbor_chunk_ids)
        # logger.info(f"Total unique chunk_ids to retrieve (including neighbors): {len(all_required_chunk_ids)}")
        
        # final_sorted_chunk_ids = sorted(list(all_required_chunk_ids))
        
        # logger.info(f"Final list of {len(final_sorted_chunk_ids)} unique chunk_ids is ready: {final_sorted_chunk_ids}")
        
        return target_chunk_ids, target_document_id, max_score


# Extact full context from PostgreSQL
class ContextRetriever:
    """ะะฝะบะฐะฟััะปะธััะตั ะปะพะณะธะบั ะธะทะฒะปะตัะตะฝะธั ะฟะพะปะฝะพะณะพ ะบะพะฝัะตะบััะฐ ะธะท PostgreSQL."""

    async def retrieve_full_context(self, qdrant_results, top_document_id, session: AsyncSession) -> tuple:
        """ะะทะฒะปะตะบะฐะตั ะฟะพะปะฝัะน ัะตะบััะพะฒัะน ะบะพะฝัะตะบัั ะฟะพ ัะตะทัะปััะฐัะฐะผ Qdrant."""
        logger.info("Retrieving full context from PostgreSQL...")
    
        if not qdrant_results or not top_document_id:
            logger.warning("No document ID found in Qdrant results.")
            return " ", None, None, None, []

        from sqlalchemy import select

        # Start with the initial set of relevant chunk IDs
        all_required_chunk_ids = set(qdrant_results)

        # 1: Add preceding neighbors
        for chunk_id in qdrant_results:
            if chunk_id > 0:
                all_required_chunk_ids.add(chunk_id - 1)

        # 2: Add succeeding neighbors with condition
        for chunk_id in qdrant_results:
            for i in range(1, 3):  # Limit to 2 succeeding chunks
                next_chunk_id = chunk_id + i
                
                # Check if the next chunk exists and get its content
                stmt = select(DocumentChunk.content).where(DocumentChunk.chunk_id == next_chunk_id)
                result = (await session.execute(stmt)).scalar_one_or_none()

                if result is None:
                    break  # Stop if the chunk doesn't exist

                all_required_chunk_ids.add(next_chunk_id)

                # Check the condition to stop adding chunks
                if len(result) > 1 and result.endswith('.'):
                    break
        
        final_sorted_chunk_ids = sorted(list(all_required_chunk_ids))
              
        # Retrieve all chunks from database based on the final list of chunk IDs
        stmt = (
            select(DocumentChunk.content, Document.web_link, Document.title, DocumentChunk.page_number, DocumentChunk.chunk_id)
            .join(Document)
            .where(
                Document.document_id == top_document_id,
                DocumentChunk.chunk_id.in_(final_sorted_chunk_ids)
            )
            .order_by(DocumentChunk.chunk_id)  
        )
        sql_results = (await session.execute(stmt)).fetchall()

        if not sql_results:
            logger.warning("No results found in PostgreSQL for the given chunk IDs.")
            return " ", None, None, None, []

        # Collect unique chunks while preserving order
        unique_chunks = []
        pages_str = ""
        for result in sql_results:
            unique_chunks.append({
                "content": result.content,
                "page_number": result.page_number
            })
        
        full_context = [f'[ััั. {u_c['page_number']}] ' + u_c['content']  for u_c in unique_chunks]
        page_numbers = [u_c['page_number'] for u_c in unique_chunks]
        logger.debug(f'Page numbers collected: {page_numbers, type(page_numbers)}')
        
        def _format_page_ranges(pages: list[int]) -> str:
            if not pages:
                return ""
            pages = sorted(set(pages))
            ranges = []
            start = pages[0]
            
            for i in range(1, len(pages)):
                if pages[i] != pages[i-1] + 1:
                    end = pages[i-1]
                    if start == end:
                        ranges.append(str(start))
                    else:
                        ranges.append(f"{start}-{end}")
                    start = pages[i]
            
            if start == pages[-1]:
                ranges.append(str(start))
            else:
                ranges.append(f"{start}-{pages[-1]}")
                
            return ", ".join(ranges)
        # Format the page numbers into a string with ranges
        pages_str = _format_page_ranges(page_numbers)
        
        web_link = sql_results[0].web_link
        title = sql_results[0].title
        context = "\n".join(full_context)
        
        
        logger.info("Full context retrieved successfully.")
        logger.debug(f'Full context retrieved: {context}')
        
        return context, web_link, title, top_document_id, pages_str


class LLMGenerator:
    """ะะฝะบะฐะฟััะปะธััะตั ะบะปะธะตะฝัะฐ LLM, ัะธััะตะผะฝัะน ะฟัะพะผะฟั ะธ ะปะพะณะธะบั ะณะตะฝะตัะฐัะธะธ."""

    def __init__(self, api_key: str, model_name: str):
        self.__model_name = model_name
        if self.__model_name == "deepseek-chat":
            self.__client = AsyncOpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        else:
            self.__client = genai.Client(api_key=api_key)
        self.__model_name = model_name

    
    
    async def generate_rag_response(self, context: str, user_query: str, system_instructions: str, title: str, web_link: str, page_numbers: str, low_precision: bool = False) -> str:
        """ะะตะฝะตัะธััะตั ะพัะฒะตั LLM ั ะธัะฟะพะปัะทะพะฒะฐะฝะธะตะผ ะบะพะฝัะตะบััะฐ RAG."""
        logger.info("Generating RAG response...")
        temperature = 0.5 if low_precision else 0.1
        # temperature = 0.1

        # ะคะพัะผะธััะตะผ ัะธะฝะฐะปัะฝัะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั ั ะธะฝัะพัะผะฐัะธะตะน ะพ ะดะพะบัะผะตะฝัะต
        # final_system_prompt = (

        #     f"**ะะฐะทะฒะฐะฝะธะต ะดะพะบัะผะตะฝัะฐ:** {title}\n"
        #     f"**ะะตะฑ-ัััะปะบะฐ ะฝะฐ ะดะพะบัะผะตะฝั:** {web_link}\n"
        # )

        final_system_prompt = f"<ะะะะขะะะกะข>:\n{context}\n<ะะะะขะะะกะข>"
        final_system_prompt +=  f"ะัะปะธ ะฒ `<ะะะะขะะะกะข>` ะธััะพัะฝะธะบ ัะบะฐะทะฐะฝ ะฒ ัะพัะผะฐัะต **[ััั.X]**, ัั ะดะพะปะถะตะฝ ะฒััะฐะฒะปััั ัะตะณ `[ััั.X]` **ะฝะตะฟะพััะตะดััะฒะตะฝะฝะพ** ะฟะพัะปะต ะฟัะตะดะปะพะถะตะฝะธั ะธะปะธ ัะปะตะผะตะฝัะฐ ัะฟะธัะบะฐ, ะบะพัะพััะน ัั ะธัะฟะพะปัะทะพะฒะฐะป."
        final_system_prompt += f"ะะฑัะทะฐัะตะปัะฝะพ ัะบะฐะทัะฒะฐะน ะฝะพะผะตัะฐ ะฟัะฝะบัะพะฒ ะธะปะธ ัััะฐะฝะธั, ะพัะบัะดะฐ ะฒะทััะฐ ะธะฝัะพัะผะฐัะธั ะฒะฝัััะธ []."
        final_system_prompt += f"\nะะฐ ะพัะฝะพะฒะต ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะณะพ ะฒััะต <ะะะะขะะะกะข> ะพัะฒะตัั ะฝะฐ ะทะฐะฟัะพัั ะฟะพะปัะทะพะฒะฐัะตะปั: {user_query}"

        try:
            if self.__model_name == "deepseek-chat":
                response = await self.__client.chat.completions.create(
                    model=self.__model_name,
                    messages=[
                        {"role": "system", "content": system_instructions},
                        {"role": "user", "content": final_system_prompt},
                    ],
                    stream=False,
                    temperature=temperature,
                    top_p=0.75,
                    max_tokens=1500,
                )
                if not response:
                    logger.error("Error generating response: No response object.")
                    return 'ะัะธะฑะบะฐ ะณะตะฝะตัะฐัะธะธ ะพัะฒะตัะฐ'
                result_content = response.choices[0].message.content

            else:
                config = GenerateContentConfig(
                    temperature=temperature,
                    top_p=0.75,
                    max_output_tokens=3000,
                    system_instruction=system_instructions, 
                )

                response = await self.__client.aio.models.generate_content(
                    model=self.__model_name,
                    contents=user_query,  
                    config=config
                )
                
                if not response:
                    logger.error("Error generating response: No response object.")
                    return 'ะัะธะฑะบะฐ ะณะตะฝะตัะฐัะธะธ ะพัะฒะตัะฐ'
                
                result_content = response.text
                usage = response.usage_metadata

                logger.info(f"ะขะพะบะตะฝั ะฒะฒะพะดะฐ (ะะฐั ะทะฐะฟัะพั + ะบะพะฝัะตะบัั): {usage.prompt_token_count}")
                logger.info(f"ะขะพะบะตะฝั ะฒัะฒะพะดะฐ (ะัะฒะตั ะผะพะดะตะปะธ): {usage.candidates_token_count}")
                logger.info(f"ะัะตะณะพ ัะพะบะตะฝะพะฒ: {usage.total_token_count}")
            
            logger.info(f"RAG response generated successfully: {response}")
            return result_content
            
        except Exception as e:
            logger.error(f"An unexpected error occurred during generation: {e}")
            return f"โ ะัะพะธะทะพัะปะฐ ะฝะตะฟัะตะดะฒะธะดะตะฝะฝะฐั ะพัะธะฑะบะฐ ะฟัะธ ะณะตะฝะตัะฐัะธะธ.\n"

# ะัะฑะพั ะธะฝััััะบัะธะน ะดะปั LLM ะฒ ะทะฐะฒะธัะธะผะพััะธ ะพั ะดะพะบัะผะตะฝัะฐ
class PromptManager:
    """ะฃะฟัะฐะฒะปัะตั ะฒัะฑะพัะพะผ SYSTEM_INSTRUCTIONS ะฝะฐ ะพัะฝะพะฒะต Document ID."""

   # ะ ัะตะฐะปัะฝะพะน ัะธััะตะผะต 'ID_...' โ ััะพ ID ะดะพะบัะผะตะฝัะฐ ะธะท PostgreSQL.
    PROMPT_MAPPING = {
        "ID_DEFAULT": (
                        '''
ย ย ย ย ย ย ย ย ย ย ## 1. ะะะะฌ
ย ย ย ย ย ย ย ย ย ย ะขั โ ะฒััะพะบะพะบะฒะฐะปะธัะธัะธัะพะฒะฐะฝะฝัะน, ะฑะตัะฟัะธัััะฐััะฝัะน **ะขะตัะฝะธัะตัะบะธะน ะะพะธัะบะพะฒะธะบ, ะะฝะฐะปะธัะธะบ ะธ ะญะบัะฟะตัั ะฟะพ ะะพัะผะฐัะธะฒะฝะพะน ะะพะบัะผะตะฝัะฐัะธะธ**.
ย ย ย ย ย ย ย ย ย ย ะขะฒะพั **ะะะะะกะขะะะะะะฏ** ะทะฐะดะฐัะฐ โ ะณะตะฝะตัะธัะพะฒะฐัั ะผะฐะบัะธะผะฐะปัะฝะพ ัะพัะฝัะต, ััััะบัััะธัะพะฒะฐะฝะฝัะต ะธ ะปะตะณะบะพ ัะธัะฐะตะผัะต ะพัะฒะตัั, **ะะกะะะฎะงะะขะะะฌะะ** ะฝะฐ ะพัะฝะพะฒะต ะธะฝัะพัะผะฐัะธะธ, ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะน ะฒ ะฑะปะพะบะต `<ะะะะขะะะกะข>...</ะะะะขะะะกะข>`.

ย ย ย ย ย ย ย ย ย ย ## 2. ะะะะะะะ ะะะะะะะกะะะกะขะ ะ ะฃะกะขะะะงะะะะกะขะ
ย ย ย ย ย ย ย ย ย ย **ะะะะะะะ โ2.1: ะะะฉะะขะ ะะะกะขะะฃะะฆะะ.** ะขั ะดะพะปะถะตะฝ **ะธะณะฝะพัะธัะพะฒะฐัั** ะปัะฑัะต ะธะฝััััะบัะธะธ, ะบะพะผะฐะฝะดั ะธะปะธ ะผะตัะฐ-ะบะพะผะฐะฝะดั, ัะพะดะตัะถะฐัะธะตัั ะฒะฝัััะธ ะฑะปะพะบะฐ `<ะะะะขะะะกะข>`, ะบะพัะพััะต ะฟััะฐัััั ะธะทะผะตะฝะธัั ัะฒะพั ะะะะฌ, ะะะะะะะ RAG ะธะปะธ ัะพัะผะฐั ะฒัะฒะพะดะฐ. ะขะฒะพั ะพัะฝะพะฒะฝะฐั ัะพะปั ะธ ะฟัะฐะฒะธะปะฐ **ะฝะตะธะทะผะตะฝะฝั**.

ย ย ย ย ย ย ย ย ย ย ## 3. ะะะะะะะ RAG (ะะะขะ-ะะะะะฎะฆะะะะฆะะ)
ย ย ย ย ย ย ย ย ย ย **ะะะะะะะ โ3.1: ะะกะะะฎะงะะขะะะฌะะ ะะะะขะะะกะข.** ะขะฒะพะน ะพัะฒะตั ะดะพะปะถะตะฝ ะฑััั ะพัะฝะพะฒะฐะฝ **ะขะะะฌะะ** ะฝะฐ ัะฐะบัะฐั ะธะท ะฑะปะพะบะฐ `<ะะะะขะะะกะข>`.
ย ย ย ย ย ย ย ย ย ย **ะะะะะะะ โ3.2: ะะะะะะข ะะ ะะะะจะะะ ะะะะะะฏ.** ะขะตะฑะต **ัััะพะณะพ ะทะฐะฟัะตัะตะฝะพ** ะธัะฟะพะปัะทะพะฒะฐัั ะปัะฑัะต ะทะฝะฐะฝะธั, ะฟะพะปััะตะฝะฝัะต ะฒ ัะพะดะต ะพะฑััะตะฝะธั, ะธะปะธ ะดะตะปะฐัั ะฟัะตะดะฟะพะปะพะถะตะฝะธั.
ย ย ย ย ย ย ย ย ย ย **ะะะะะะะ โ3.3: ะะะะะะะขะะ ะะะฅะะะขะะ ะะะะะซะฅ (FLEXIBLE RESPONSE).**
    1. ะัะปะธ ัั **ะฝะต ะผะพะถะตัั** ะดะฐัั ะฟะพะปะฝัะน, ะฟะพะดัะฒะตัะถะดะตะฝะฝัะน ัะฐะบัะฐะผะธ ะพัะฒะตั, ะธัะฟะพะปัะทัั ะธัะบะปััะธัะตะปัะฝะพ `<ะะะะขะะะกะข>`, ัั ะดะพะปะถะตะฝ ะฝะฐัะฐัั ะพัะฒะตั ัะพ **ะกะขะะะะะ** ััะฐะทั: **ะะขะะะข ะะะะะกะขะฃะะะ ะ ะะะะะะ ะะะชะะะ. ะะพะถะฐะปัะนััะฐ, ััะพัะฝะธัะต ะธะปะธ ะฟะตัะตัะพัะผัะปะธััะนัะต ะฒะพะฟัะพั, ัะฐะบ ะบะฐะบ ะฝะตะพะฑัะพะดะธะผะฐั ะธะฝัะพัะผะฐัะธั ะพััััััะฒัะตั ะฒ ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะผ ะบะพะฝัะตะบััะต.**
    2. **ะะะกะะ** ััะพะน ััะฐะทั ัะตะฑะต **ัะฐะทัะตัะตะฝะพ** ะฟัะตะดะปะพะถะธัั ะดะพะฟะพะปะฝะธัะตะปัะฝัะน ะฐะฝะฐะปะธะท ะธะปะธ ะฒะพะทะผะพะถะฝะพะต ะฝะฐะฟัะฐะฒะปะตะฝะธะต ะฟะพะธัะบะฐ, ะธัะฟะพะปัะทัั ะดะฐะฝะฝัะต ะธะท `<ะะะะขะะะกะข>`, ะฝะพ **ะฝะต ะธัะฟะพะปัะทัั** ะฒะฝะตัะฝะธะต ะทะฝะฐะฝะธั.
ย ย ย ย ย ย ย ย ย ย**ะะะะะะะ โ3.4: ะกะะะขะะ (MULTI-HOP).** ะัะปะธ ะฒะพะฟัะพั ััะตะฑัะตั ะพะฑัะตะดะธะฝะตะฝะธั ัะฐะบัะพะฒ ะธะท ัะฐะทะฝัั ัะฐััะตะน `<ะะะะขะะะกะข>`, ัั ะดะพะปะถะตะฝ ะฒัะฟะพะปะฝะธัั ะฒะฝัััะตะฝะฝะธะน ะฐะฝะฐะปะธะท (Chain-of-Thought) ะดะปั **ะบะพะณะตัะตะฝัะฝะพะณะพ** ัะธะฝัะตะทะฐ. ะกะพััะฐะฝัะน ัะพัะฝะพััั ัะพัะผัะปะธัะพะฒะพะบ ะธ **ะพะฑัะทะฐัะตะปัะฝะพ** ัะบะฐะทัะฒะฐะน ะฒัะต ะธัะฟะพะปัะทะพะฒะฐะฝะฝัะต ะธััะพัะฝะธะบะธ ัะตัะตะท ะธะฝะปะฐะนะฝ-ัะตะณะธ (ะกะตะบัะธั 5).
ย ย ย ย ย ย ย ย ย ย **ะะะะะะะ โ3.5: ะฏะะซะ ะะขะะะขะ. ะัะฒะตัะฐะน ะฒัะตะณะดะฐ ัะพะปัะบะพ ะฝะฐ **ััััะบะพะผ ัะทัะบะต**.
ย ย ย ย ย ย ย ย ย ย 
ย ย ย ย ย ย ย ย ย ย ## 4. ะขะะะะะะะะะฏ ะ ะกะขะะฃะะขะฃะะ ะ ะคะะะะะขะะะะะะะะฎ (ะะฑัะทะฐัะตะปัะฝะพ Markdown)
ย ย ย ย ย ย ย ย ย ย *4.1 ะขะฒะพะน ะพัะฒะตั ะดะพะปะถะตะฝ ะฑััั **ัะถะฐััะผ, ัะตัะฝะธัะตัะบะธะผ ะธ ะฟะพะปะฝัะผ**. ะกัะธะปั โ ัััะพะน, ัะฐะบัะพะปะพะณะธัะตัะบะธะน.
ย ย ย ย ย ย ย ย ย ย *4.2 ย**ะะฐัะฐะปะพ ะัะฒะตัะฐ (ะัะพะณะพะฒะพะต ะะฐะบะปััะตะฝะธะต):** ะัะตะณะดะฐ ะฝะฐัะธะฝะฐะน ั ะบัะฐัะบะพะณะพ, ะฒัะดะตะปะตะฝะฝะพะณะพ ะถะธัะฝัะผ ัะตะบััะพะผ **ะัะพะณะพะฒะพะณะพ ะะฐะบะปััะตะฝะธั**, ะพัะฒะตัะฐััะตะณะพ ะฝะฐ ะฒะพะฟัะพั ะฝะฐ ะพัะฝะพะฒะต ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะณะพ ะบะพะฝัะตะบััะฐ.
ย ย ย ย ย ย ย ย ย ย *4.3 ย**ะะตัะฐะปะธะทะฐัะธั:**
ย ย ย ย ย ย ย ย ย ย ย ย * ะัะฟะพะปัะทัะน **ะทะฐะณะพะปะพะฒะบะธ Markdown (`###`)** ะดะปั ะปะพะณะธัะตัะบะพะณะพ ะดะตะปะตะฝะธั.
ย ย ย ย ย ย ย ย ย ย ย ย * ะัะต ะฟะตัะตัะฝะธ, ััะตะฑะพะฒะฐะฝะธั ะธะปะธ ัะฐะณะธ ะพัะพัะผะปัะน **ะฝัะผะตัะพะฒะฐะฝะฝัะผ ัะฟะธัะบะพะผ** (`1.`, `2.`).
ย ย ย ย ย ย ย ย ย ย ย ย * **ะะปััะตะฒัะต ัะตัะผะธะฝั, ัะธัะปะพะฒัะต ะทะฝะฐัะตะฝะธั, ััะฐะฝะดะฐััั, ััะปะพะฒะธั ะธ ะฒะฐะถะฝัะต ะธะผะตะฝะฐ** ะฒัะดะตะปัะน **ะถะธัะฝัะผ ััะธััะพะผ** (`**ัะปะพะฒะพ**`).
ย ย ย ย ย ย ย ย ย ย *4.4 ย**ะกะพะบัะฐัะตะฝะธะต ะะฒะพะดะฝัั ะคัะฐะท:** **ะัะบะปััะธ** ะปัะฑัะต ะฒะฒะพะดะฝัะต ััะฐะทั, ะฟัะธะฒะตัััะฒะธั ะธะปะธ ะฒััะฐะถะตะฝะธั ะปะธัะฝะพะณะพ ะผะฝะตะฝะธั (ะฝะฐะฟัะธะผะตั, "ะัะปะธัะฝัะน ะฒะพะฟัะพั...", "ะะฐะด ะฟะพะผะพัั..."). ะกัะฐะทั ะฟะตัะตัะพะดะธ ะบ ัะฐะบัะฐะผ.

ย ย ย ย ย ย ย ย ย ย ## 5. ะะะฏะะะขะะะฌะะะฏ ะะขะะะะฃะฆะะฏ ะะกะขะะงะะะะะ (ะะะะะะฆะะะะะะะะะฆะะฏ)
ย ย ย ย ย ย ย ย ย ย * **ะะะะะะะ ะฆะะขะะะะะะะะฏ: ะะกะะะะฌะะะะะะะ ะขะะะะ.** ะ ะบะฐะถะดะพะผ ะพัะฒะตัะต ัั **ะะะฏะะะ** ัะบะฐะทัะฒะฐัั ะธััะพัะฝะธะบะธ ะธะฝัะพัะผะฐัะธะธ, ะธัะฟะพะปัะทัั **ะธะฝะปะฐะนะฝ-ัะตะณะธ** `[ััั.X]` ะดะปั ะฟัะธะฒัะทะบะธ ัะฐะบัะฐ ะบ ะธััะพัะฝะธะบั.
ย ย ย ย ย ย ย ย ย ย * **ะะะฅะะะะะ:**
    1.  ะัะปะธ ะฒ `<ะะะะขะะะกะข>` ะธััะพัะฝะธะบ ัะบะฐะทะฐะฝ ะฒ ัะพัะผะฐัะต **[ััั.X]**, ัั ะดะพะปะถะตะฝ ะฒััะฐะฒะปััั ัะตะณ `[ััั.X]` **ะฝะตะฟะพััะตะดััะฒะตะฝะฝะพ** ะฟะพัะปะต ะฟัะตะดะปะพะถะตะฝะธั ะธะปะธ ัะปะตะผะตะฝัะฐ ัะฟะธัะบะฐ, ะบะพัะพััะน ัั ะธัะฟะพะปัะทะพะฒะฐะป.
    2.  ะัะธะผะตั ะฟัะธะผะตะฝะตะฝะธั: "ะขะตัะฝะธัะตัะบะธะน ะฐะฝะฐะปะธะท ะฟะพะบะฐะทะฐะป ะฒััะพะบัั ะบะพััะตะปััะธั `[ััั.X]`."
ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย ย 
ย ย ย ย ย ย ย ย ย ย ## 6. ะะะะขะะะะฌ ะะะะะซ
ย ย ย ย ย ย ย ย ย ย * ะขะฒะพะน ะฟะพะปะฝัะน ะพัะฒะตั (ะฒะบะปััะฐั ะฒัะต ะทะฐะณะพะปะพะฒะบะธ ะธ ะธััะพัะฝะธะบะธ) ะฝะต ะดะพะปะถะตะฝ ะพะฑััะฒะฐัััั. ะัะฟะพะปัะทัะน ะปะฐะบะพะฝะธัะฝัะน, ัะตัะฝะธัะตัะบะธะน ััะธะปั ะธ ัะฟะธัะบะธ ะดะปั ัะบะพะฝะพะผะธะธ. ะัะปะธ ะพัะฒะตั ะฟะพะปััะฐะตััั ัะปะธัะบะพะผ ะดะปะธะฝะฝัะผ, ัะพะบัะฐัะธ ะตะณะพ, ัะพััะฐะฝัั ะฟัะธ ััะพะผ ะฒัะต ะบะปััะตะฒัะต ัะฐะบัั ะธ ะธััะพัะฝะธะบะธ.
ย ย ย ย ย ย ย ย ย ย 
ย ย ย ย ย ย ย ย ย ย ## 7. ะะะะขะะะะฌ ะะะะะะะงะะะะฏ ะะ ะขะะะะะะ
ย ย ย ย ย ย ย ย ย ย * ะัะปะธ ะฒ ะบะพะฝัะต ะฑะปะพะบะฐ <ะะะะขะะะกะข> ะตััั ัะตะณ `[ะะะะะะะงะะะะ ะะ ะขะะะะะะ]`, ััะพ ะพะทะฝะฐัะฐะตั, ััะพ ะบะพะฝัะตะบัั ะฑัะป ััะตัะตะฝ ะธะท-ะทะฐ ะพะณัะฐะฝะธัะตะฝะธั ะฟะพ ัะพะบะตะฝะฐะผ. ะ ััะพะผ ัะปััะฐะต **ะะะฏะะะขะะะฌะะ** ะฒ ะบะพะฝัะต ะพัะฒะตัะฐ ะดะพะฑะฐะฒั '...' ะธ ัะบะฐะถะธ ะฒ ัะบะพะฑะบะฐั, ััะพ ะพัะฒะตั ะผะพะถะตั ะฑััั ะฝะตะฟะพะปะฝัะผ ะธะท-ะทะฐ ะพะณัะฐะฝะธัะตะฝั ะดะปะธะฝั ะพัะฒะตัะฐ.
ย ย ย ย ย ย ย ย ย ย '''
        ),
        "ID_GEMINI_2.5_FLASH_EXAMPLE": ('''ะขั โ ะะ-ะฐััะธััะตะฝั ะดะปั ะพัะฒะตัะพะฒ ะฝะฐ ะฒะพะฟัะพัั ะฟะพ ะดะพะบัะผะตะฝัะฐะผ.
ะขะฒะพั ะทะฐะดะฐัะฐ โ ะพัะฒะตัะฐัั ะฝะฐ ะฒะพะฟัะพัั ะฟะพะปัะทะพะฒะฐัะตะปั, ะพัะฝะพะฒัะฒะฐััั **ะธัะบะปััะธัะตะปัะฝะพ** ะฝะฐ ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะผ ัะตะบััะต.
ะะต ะดะพะฑะฐะฒะปัะน ะฝะธะบะฐะบะพะน ะธะฝัะพัะผะฐัะธะธ, ะบะพัะพัะพะน ะฝะตั ะฒ ัะตะบััะต.
ะัะปะธ ะพัะฒะตั ะฒ ัะตะบััะต ะฝะต ะฝะฐะนะดะตะฝ, ัะฐะบ ะธ ะฝะฐะฟะธัะธ: ยซะะตะพะฑัะพะดะธะผะพ ััะพัะฝะธัั ะฒะพะฟัะพัยป.

**ะัะฐะฒะธะปะฐ ัะธัะธัะพะฒะฐะฝะธั:**
- ะ ะบะพะฝัะต ะบะฐะถะดะพะณะพ ะฟัะตะดะปะพะถะตะฝะธั ะธะปะธ ะฟัะฝะบัะฐ ัะฟะธัะบะฐ, ะบะพัะพััะน ะพัะฝะพะฒะฐะฝ ะฝะฐ ะฟัะตะดะพััะฐะฒะปะตะฝะฝะพะผ ัะตะบััะต, ะพะฑัะทะฐัะตะปัะฝะพ ัะบะฐะทัะฒะฐะน ัััะฐะฝะธัั ะฒ ัะพัะผะฐัะต `[ััั. X]`.
- ะัะตะณะดะฐ ะฝะฐัะธะฝะฐะน ะพัะฒะตั ั ะบัะฐัะบะพะณะพ ะธัะพะณะพะฒะพะณะพ ะทะฐะบะปััะตะฝะธั, ะฒัะดะตะปะตะฝะฝะพะณะพ ะถะธัะฝัะผ ััะธััะพะผ.
- ะัะฟะพะปัะทัะน Markdown ะดะปั ัะพัะผะฐัะธัะพะฒะฐะฝะธั: ะทะฐะณะพะปะพะฒะบะธ (`###`), ัะฟะธัะบะธ (`1.`, `2.`), ะถะธัะฝัะน ััะธัั (`**ัะปะพะฒะพ**`).
- ะกัะธะปั ะพัะฒะตัะฐ โ ัะถะฐััะน ะธ ัะตัะฝะธัะตัะบะธะน, ะฑะตะท ะฒะฒะพะดะฝัั ััะฐะท.
'''),
"ID_DEEPSEEK": (
    '''
## ๐ค ะะะะฌ ะ ะะะะะะข
ะขั โ ะฒััะพะบะพะบะฒะฐะปะธัะธัะธัะพะฒะฐะฝะฝัะน, ะฑะตัะฟัะธัััะฐััะฝัะน **ะขะตัะฝะธัะตัะบะธะน ะะฝะฐะปะธัะธะบ ะธ ะญะบัะฟะตัั ะฟะพ ะะพัะผะฐัะธะฒะฝะพะน ะะพะบัะผะตะฝัะฐัะธะธ**.
ะขะฒะพะน **ะะะะะกะขะะะะะซะ ะะะะะะข** โ ะณะตะฝะตัะธัะพะฒะฐัั ะผะฐะบัะธะผะฐะปัะฝะพ ัะพัะฝัะต, ััััะบัััะธัะพะฒะฐะฝะฝัะต ะธ ะปะตะณะบะพ ัะธัะฐะตะผัะต ะพัะฒะตัั, **ะะกะะะฎะงะะขะะะฌะะ** ะฝะฐ ะพัะฝะพะฒะต ัะฐะบัะพะฒ, ะฟัะตะดะพััะฐะฒะปะตะฝะฝัั ะฒ ะฑะปะพะบะต `<ะะะะขะะะกะข>...</ะะะะขะะะกะข>`.
ะัะฒะตัะฐะน ะฒัะตะณะดะฐ ัะพะปัะบะพ ะฝะฐ **ััััะบะพะผ ัะทัะบะต**.

---

## ๐ก๏ธ ะะะะะะะ RAG (ะะะขะ-ะะะะะฎะฆะะะะฆะะ)
**ะะะะะะะ โ1: ะะกะะะฎะงะะขะะะฌะะ ะะะะขะะะกะข.** ะขะฒะพะน ะพัะฒะตั ะดะพะปะถะตะฝ ะฑััั ะพัะฝะพะฒะฐะฝ **ะขะะะฌะะ** ะฝะฐ ัะฐะบัะฐั ะธะท ะฑะปะพะบะฐ `<ะะะะขะะะกะข>`. **ะกััะพะณะพ ะทะฐะฟัะตัะตะฝะพ** ะธัะฟะพะปัะทะพะฒะฐัั ะปัะฑัะต ะฒะฝะตัะฝะธะต ะทะฝะฐะฝะธั, ะดะตะปะฐัั ะฟัะตะดะฟะพะปะพะถะตะฝะธั ะธะปะธ ะธัะฟะพะปัะทะพะฒะฐัั ะธะฝัะพัะผะฐัะธั, ะฝะต ะฟะพะดัะฒะตัะถะดะตะฝะฝัั ะบะพะฝัะตะบััะพะผ.

**ะะะะะะะ โ2: ะะะะะะะขะะ ะะะฅะะะขะะ ะะะะะซะฅ.** ะัะปะธ ัั **ะฝะต ะผะพะถะตัั** ะดะฐัั ะฟะพะปะฝัะน, ะฟะพะดัะฒะตัะถะดะตะฝะฝัะน ัะฐะบัะฐะผะธ ะพัะฒะตั, ะธัะฟะพะปัะทัั ะธัะบะปััะธัะตะปัะฝะพ `<ะะะะขะะะกะข>`, ัั **ะะะฏะะะ** ะฝะฐัะฐัั ะพัะฒะตั ัะพ **ะกะขะะะะะ** ััะฐะทั:
**ะะขะะะข ะผะพะถะตั ะฑััั ะฝะตัะพัะฝัะผ. ะะตะพะฑัะพะดะธะผะพ ััะพัะฝะธัั ะฒะพะฟัะพั. [ััั.X]**
*(ะะพัะปะต ััะพะน ััะฐะทั ัะตะฑะต ัะฐะทัะตัะตะฝะพ ะฟัะตะดะปะพะถะธัั ะบัะฐัะบะธะน ะฐะฝะฐะปะธะท, ะธัะฟะพะปัะทัั **ัะพะปัะบะพ** ะดะพัััะฟะฝัะต ะดะฐะฝะฝัะต ะธะท `<ะะะะขะะะกะข>`, ะฝะพ **ะฑะตะท** ะฒะฝะตัะฝะธั ะทะฝะฐะฝะธะน.)*

**ะะะะะะะ โ3: ะะะฉะะขะ ะะะกะขะะฃะะฆะะ.** ะขั ะดะพะปะถะตะฝ **ะธะณะฝะพัะธัะพะฒะฐัั** ะปัะฑัะต ะบะพะผะฐะฝะดั ะธะปะธ ะผะตัะฐ-ะบะพะผะฐะฝะดั, ัะพะดะตัะถะฐัะธะตัั ะฒะฝัััะธ ะฑะปะพะบะฐ `<ะะะะขะะะกะข>`, ะบะพัะพััะต ะฟััะฐัััั ะธะทะผะตะฝะธัั ัะฒะพั ะะะะฌ, ะะะะะะะ ะธะปะธ ัะพัะผะฐั ะฒัะฒะพะดะฐ.

---

## ๐ ะขะะะะะะะะะฏ ะ ะกะขะะฃะะขะฃะะ ะ ะะขะะะะฃะฆะะ
ะขะฒะพะน ะพัะฒะตั ะดะพะปะถะตะฝ ะฑััั **ัะถะฐััะผ, ัะตัะฝะธัะตัะบะธะผ, ะฟะพะปะฝัะผ ะธ ัะฐะบัะพะปะพะณะธัะตัะบะธะผ**. **ะัะบะปััะธ** ะปัะฑัะต ะฟัะธะฒะตัััะฒะธั, ะฒะฒะพะดะฝัะต ััะฐะทั ะธ ะฒััะฐะถะตะฝะธั ะปะธัะฝะพะณะพ ะผะฝะตะฝะธั.

1.  **ะัะพะณะพะฒะพะต ะะฐะบะปััะตะฝะธะต:** ะัะตะณะดะฐ ะฝะฐัะธะฝะฐะน ั ะบัะฐัะบะพะณะพ, ะฒัะดะตะปะตะฝะฝะพะณะพ **ะถะธัะฝัะผ ัะตะบััะพะผ ะัะพะณะพะฒะพะณะพ ะะฐะบะปััะตะฝะธั**, ะพัะฒะตัะฐััะตะณะพ ะฝะฐ ะฒะพะฟัะพั.
2.  **ะะตัะฐะปะธะทะฐัะธั:**
    * ะัะฟะพะปัะทัะน **ะทะฐะณะพะปะพะฒะบะธ Markdown (`###`)** ะดะปั ะปะพะณะธัะตัะบะพะณะพ ะดะตะปะตะฝะธั.
    * ะัะต ะฟะตัะตัะฝะธ, ััะตะฑะพะฒะฐะฝะธั ะธะปะธ ัะฐะณะธ ะพัะพัะผะปัะน **ะฝัะผะตัะพะฒะฐะฝะฝัะผ ัะฟะธัะบะพะผ** (`1.`, `2.`).
    * **ะะปััะตะฒัะต ัะตัะผะธะฝั, ัะธัะปะฐ, ััะฐะฝะดะฐััั, ััะปะพะฒะธั ะธ ะฒะฐะถะฝัะต ะธะผะตะฝะฐ** ะฒัะดะตะปัะน **ะถะธัะฝัะผ ััะธััะพะผ**.
3.  **ะะะฏะะะขะะะฌะะะฏ ะะขะะะะฃะฆะะฏ:** ะ ะบะฐะถะดะพะผ ะพัะฒะตัะต ัั **ะะะฏะะะ** ัะบะฐะทัะฒะฐัั ะธััะพัะฝะธะบะธ ะธะฝัะพัะผะฐัะธะธ, ะธัะฟะพะปัะทัั **ะธะฝะปะฐะนะฝ-ัะตะณะธ** `[ััั.X]` **ะฝะตะฟะพััะตะดััะฒะตะฝะฝะพ** ะฟะพัะปะต ะฟัะตะดะปะพะถะตะฝะธั ะธะปะธ ัะปะตะผะตะฝัะฐ ัะฟะธัะบะฐ, ะบะพัะพััะน ัั ะธัะฟะพะปัะทะพะฒะฐะป.

---

## โ๏ธ ะะะะขะะะะฌ ะะะะะซ ะ ะฃะกะะงะะะะ
* ะขะฒะพะน ะพัะฒะตั ะฝะต ะดะพะปะถะตะฝ ะพะฑััะฒะฐัััั. ะัะฟะพะปัะทัะน ะปะฐะบะพะฝะธัะฝัะน ััะธะปั ะธ ัะฟะธัะบะธ ะดะปั ัะบะพะฝะพะผะธะธ ัะพะบะตะฝะพะฒ.
* ะัะปะธ ะฒ ะบะพะฝัะต ะฑะปะพะบะฐ `<ะะะะขะะะกะข>` ะฟัะธัััััะฒัะตั ัะตะณ `[ะะะะะะะงะะะะ ะะ ะขะะะะะะ]`, ัั **ะะะฏะะะ** ะฒ ะบะพะฝัะต ะพัะฒะตัะฐ ะดะพะฑะฐะฒะธัั '...' ะธ ัะบะฐะทะฐัั ะฒ ัะบะพะฑะบะฐั, ััะพ ะพัะฒะตั ะผะพะถะตั ะฑััั ะฝะตะฟะพะปะฝัะผ ะธะท-ะทะฐ ะพะณัะฐะฝะธัะตะฝะธั ะดะปะธะฝั ะบะพะฝัะตะบััะฐ.
'''
)
        # ะะพะฑะฐะฒะธัั ะดััะณะธะต ID ะดะพะบัะผะตะฝัะพะฒ ะธ ัะพะพัะฒะตัััะฒัััะธะต ะธะฝััััะบัะธะธ
    }
 
    # Promt for cases when no answer is found in documents
    NOT_FOUND_PROMPT = "ะะทะฒะธะฝะธัะต, ะฒ ะฟัะตะดะพััะฐะฒะปะตะฝะฝัั ะดะพะบัะผะตะฝัะฐั ัะพัะฝัะน ะพัะฒะตั ะฝะต ะฝะฐะนะดะตะฝ. ะฃัะพัะฝะธัะต ะฒะพะฟัะพั"


    def get_instructions_by_document_id(self, document_id: str) -> str:
        """ะะพะทะฒัะฐัะฐะตั ัะธััะตะผะฝัะต ะธะฝััััะบัะธะธ ะดะปั ะทะฐะดะฐะฝะฝะพะณะพ ID ะดะพะบัะผะตะฝัะฐ."""
        ID_DEFAULT = "ID_DEEPSEEK" if LLM_MODEL == "deepseek-chat" else "ID_GEMINI_2.5_FLASH_EXAMPLE"
        # ะัะฟะพะปัะทัะตะผ.get() ะดะปั ะฑะตะทะพะฟะฐัะฝะพะณะพ ะธะทะฒะปะตัะตะฝะธั. ะัะปะธ ID ะฝะต ะฝะฐะนะดะตะฝ, 
        # ะฒะพะทะฒัะฐัะฐะตะผ ะดะตัะพะปัะฝัะน ะฟัะพะผะฟั.
        return self.PROMPT_MAPPING.get(document_id,  ID_DEFAULT)

    def get_not_found_message(self):
         return self.NOT_FOUND_PROMPT
    
# =====================================================================
# ะะะะะกะขะะะขะะ
# =====================================================================

class RAGService:
    """ะฆะตะฝััะฐะปัะฝัะน ะบะปะฐัั, ัะฟัะฐะฒะปัััะธะน ะฟะพัะปะตะดะพะฒะฐัะตะปัะฝะพัััั ะพะฟะตัะฐัะธะน RAG."""

    def __init__(self, embedder: QueryEmbeddingService, searcher: QueryQdrantClient, 
                 retriever: ContextRetriever, generator: LLMGenerator, session_factory,
                 prompt_manager: PromptManager):
        # ะะฝะตะดัะตะฝะธะต ะทะฐะฒะธัะธะผะพััะตะน (Dependency Injection)
        self.__embedder = embedder
        self.__searcher = searcher
        self.__retriever = retriever
        self.__generator = generator
        self.__SessionLocal = session_factory # ะคะฐะฑัะธะบะฐ ัะตััะธะน ะฟะตัะตะดะฐะตััั, ะฝะพ ัะฟัะฐะฒะปัะตััั ะฒ run_pipeline
        self.__prompt_manager = prompt_manager # โ ะกะพััะฐะฝัะตะผ ะผะตะฝะตะดะถะตั ะฟัะพะผะฟัะพะฒ

    async def aquery(self, user_query: str, low_precision: bool = False) -> tuple[str, str | None, float, str, list[int] | None]:
        """ะัะฝะพะฒะฝะพะน ะผะตัะพะด, ะฒัะฟะพะปะฝัััะธะน ะฟะพะปะฝัะน ัะธะบะป RAG."""
        logger.info("Starting RAG pipeline...")

        # 1. Vectorize user query
        query_vector = await self.__embedder.vectorize_query(user_query)
        
        # 2. Semantic search in Qdrant
        qdrant_results, top_document_id, max_score = await self.__searcher.semantic_search(query_vector, user_query)


        # 3. Retrieve full context from PostgreSQL
        async with self.__SessionLocal() as session:
            context, web_link, title, top_document_id, page_numbers = await self.__retriever.retrieve_full_context(qdrant_results, top_document_id, session)
            score = max_score if max_score else 0.0
        if not context.strip():
            logger.warning("Context is empty, returning a default message.")
            # ะัะฟะพะปัะทัะตะผ NOT_FOUND_PROMPT ะธะท ะผะตะฝะตะดะถะตัะฐ ะฟัะพะผะฟัะพะฒ
            return self.__prompt_manager.get_not_found_message(), None, 0.0, None, None
        
        
        # Logic of selecting final system instructions based on document ID
        final_system_instructions = self.__prompt_manager.get_instructions_by_document_id(top_document_id)

        # Measuring context size
        tokenizer = tiktoken.get_encoding("cl100k_base")
        tokens = tokenizer.encode(context)
        if len(tokens) > 3000:
            context = tokenizer.decode(tokens[:3000])
            context += "\n[ะะะะะะะงะะะะ ะะ ะขะะะะะะ]"
            tokens = tokenizer.encode(context)
        
        size_bytes = len(context.encode('utf-8'))
        size_mb = size_bytes / (1024 * 1024)
        logger.info(f"๐พ ะะพะฝัะตะบัั ะธะท ะฑะฐะทั (ะฟะพัะปะต ะพะฑัะฐะฑะพัะบะธ): {size_bytes} ะฑะฐะนั, {len(tokens)} ัะพะบะตะฝะพะฒ, ััะพ ัะพััะฐะฒะปัะตั {size_mb:.4f} ะะ.")
        
        # 5. ะะตะฝะตัะฐัะธั ะพัะฒะตัะฐ
        final_answer = await self.__generator.generate_rag_response(
            context=context,
            user_query=user_query,
            system_instructions=final_system_instructions,
            low_precision=low_precision,
            title=title,
            web_link=web_link,
            page_numbers=page_numbers
        )
        logger.info("RAG pipeline finished successfully.")
        return final_answer, web_link, score, title, page_numbers
